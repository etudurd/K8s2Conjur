- name: Scan app for Kubernetes Secret usage and authenticate to Conjur
  hosts: localhost
  gather_facts: false
  vars:
    policy_template_file: conjur_host_policy_template.j2
    policy_rendered_file: /tmp/conjur_app_path_policy.yaml
    variable_policy_template_file: conjur_variables_template.j2
    variable_policy_rendered_file: /tmp/conjur_generated_variables.yaml
    secrets_provider_patch_template_file: secrets_provider_push_sidecar.j2
    secrets_provider_config_file: /tmp/updated_deployment_with_provider.yaml
    conjur_ssl_cert_path: /tmp/conjur.pem

  tasks:
    # Step 0: Fetch Conjur Leader/Follower certificate and render ConfigMap
    - name: Download Conjur public certificate
      shell: |
        openssl s_client -showcerts -connect {{ conjur_host }}:443 < /dev/null 2> /dev/null \
        | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > {{ conjur_ssl_cert_path }}
      args:
        executable: /bin/bash

    - name: Read Conjur certificate into memory
      slurp:
        src: "{{ conjur_ssl_cert_path }}"
      register: cert_raw

    - name: Create Conjur ConfigMap using k8s module with token auth
      kubernetes.core.k8s:
        state: present
        host: "https://{{ ocp_api_host }}"
        api_key: "{{ ocp_token }}"
        verify_ssl: false
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: follower-cm
            namespace: "{{ k8s_namespace }}"
          data:
            CONJUR_ACCOUNT: "{{ conjur_account }}"
            CONJUR_APPLIANCE_URL: "https://{{ conjur_host }}"
            CONJUR_SEED_FILE_URL: "https://{{ conjur_host }}/configuration/{{ conjur_account }}/seed/follower"
            AUTHENTICATOR_ID: "{{ authenticator_id }}"
            CONJUR_SSL_CERTIFICATE: "{{ cert_raw['content'] | b64decode }}"

    # Step 1: Fetch deployment details from OpenShift API
    - name: Get deployment spec
      kubernetes.core.k8s_info:
        kind: Deployment
        api_version: apps/v1
        name: "{{ deployment_name }}"
        namespace: "{{ k8s_namespace }}"
        host: "https://{{ ocp_api_host }}"
        api_key: "{{ ocp_token }}"
        verify_ssl: false
      register: dep_info

    - name: Debug if deployment exists
      debug:
        msg: "❌ No deployment found with name '{{ deployment_name }}' in namespace '{{ k8s_namespace }}'.❌"
      when: dep_info.resources | length == 0

    # Step 2: Extract secret references from the deployment, for push to file, before env.valueFrom.secretKeyRef, volumes[*].secret.secretName now check for Secrets mounted in volumes
#,ConfigMaps mounted in volumes 
    - name: Extract referenced secrets and configmaps from deployment
      set_fact:
        used_secrets: >-
          {{
            (
              dep_info.resources[0].spec.template.spec.containers
              | map(attribute='env') | map('default', []) | list | sum(start=[])
              | selectattr('valueFrom', 'defined')
              | map(attribute='valueFrom')
              | map('dict2items') | sum(start=[])
              | selectattr('value.name', 'defined')
              | map(attribute='value.name')
            )
            +
            (
              dep_info.resources[0].spec.template.spec.volumes
              | default([]) | selectattr('secret.secretName', 'defined')
              | map(attribute='secret.secretName')
            )
            +
            (
              dep_info.resources[0].spec.template.spec.volumes
              | default([]) | selectattr('configMap.name', 'defined')
              | map(attribute='configMap.name')
            )
          | list | unique }}

    # Step 3: Retrieve all secrets in namespace
    - name: Get all secrets in namespace
      kubernetes.core.k8s_info:
        kind: Secret
        api_version: v1
        namespace: "{{ k8s_namespace }}"
        host: "https://{{ ocp_api_host }}"
        api_key: "{{ ocp_token }}"
        verify_ssl: false
      register: all_secrets

    - name: Extract keys from used secrets
      set_fact:
        secret_kv_pairs: >-
          {{ all_secrets.resources | selectattr('metadata.name', 'in', used_secrets)
                                   | map(attribute='data') | list
                                   | map('dict2items') | list | sum(start=[]) }}


############################################################################################
# TBU - partea de sanitizarea de adaugat si la restul de proiecte 
    #- name: Sanitize keys (remove .yaml, .yml, .json, .txt)
      #set_fact:
        #sanitized_keys: >-
          # {{ secret_kv_pairs | map(attribute='key')
                             #| map('regex_replace', '\.(yaml|yml|json|txt)(\.backup)?$', '') | list }}

   # Create a mapping: original → sanitized (needed for both patch & upload)
    - name: Build map of original → cleaned keys
      set_fact:
        key_sanitization_map: >-
          {{
            dict(
              secret_kv_pairs | map(attribute='key') |
              zip(
                secret_kv_pairs | map(attribute='key') |
                map('regex_replace', '\.(yaml|yml|json|txt)(\.backup)?$', '')
              )
            )
          }}
#am inlocuit sanitized_keys cu key_sanitization_map 
    - name: Generate prefixed secret keys for Conjur
      set_fact:
        conjur_prefixed_secret_keys: >-
          {{ key_sanitization_map | map('regex_replace', '^(.*)$', deployment_name ~ '-OnboardedSecret/\1') | list }}

# De folosit in Push to file 

    # - name: Build sanitized secret_kv_pairs for templating
      # set_fact:
        # secret_kv_pairs_sanitized: >-
          # {{
            # secret_kv_pairs | map('combine',
              # {'key': key_sanitization_map[item.key] }) | list
          # }}
          
    - name: Build sanitized secret_kv_pairs for templating
      set_fact:
        secret_kv_pairs_sanitized: >-
          {{
            secret_kv_pairs | map('extract', ['key', 'value']) |
            map('zip', key_sanitization_map.values() | list) |
            map('map', 'reverse') |                
            map('combine') | list
          }}
# [sanitized, [key, value]] → [key, value]
#########################################################################Terminat partea de sanitizare, sa nu am .yaml uploadat in conjur si sa fac patching cu reff corect
      
    - name: Build secret name → value map with prefixed keys
      set_fact:
        conjur_prefixed_secret_map: >-
          {{ dict(
            conjur_prefixed_secret_keys | zip(
              secret_kv_pairs | map(attribute='value')
            )
          ) }}

    - name: Debug Conjur-prefixed secret map
      debug:
        var: conjur_prefixed_secret_map

    - name: Classify secrets by mount type
      set_fact:
        file_based_keys: >-
          {{
            dep_info.resources[0].spec.template.spec.volumes
            | default([])
            | selectattr('configMap.name', 'defined')
            | map(attribute='configMap.name') | list
          }}
        env_based_keys: >-
          {{
            dep_info.resources[0].spec.template.spec.containers
            | map(attribute='env') | map('default', []) | list | sum(start=[])
            | selectattr('valueFrom', 'defined')
            | map(attribute='valueFrom.secretKeyRef.name') | select('defined') | list
          }}
        onboarding_mode: all
        
    - name: Show decoded values before upload
      debug:
        msg: "{{ item.key }} → {{ item.value | b64decode }}"
      loop: >-
        {{ conjur_prefixed_secret_map | dict2items
                                     | rejectattr('value', 'equalto', '')
                                     | rejectattr('key', 'search', 'conjur-map$')
                                     | list }}

    - name: Set secret_keys for templating
      set_fact:
        secret_keys: "{{ conjur_prefixed_secret_keys }}"

    # Step 4: Suggest improvements
    - name: Suggest improvements
      debug:
        msg: |
          💡💡💡💡 Recommendations:
          {% for s in used_secrets %}
          - Secret '{{ s }}' might be a candidate for CyberArk Conjur integration.
            Consider replacing static secret with dynamic retrieval using the Secrets Provider sidecar or init container.
          {% endfor %}

    # Step 5: Authenticate to Conjur (API key + session token)
    - name: Login to Conjur (get API key)
      shell: >
        curl -s -k --user {{ conjur_username }}:{{ conjur_password }}
        https://{{ conjur_host }}/authn/{{ conjur_account }}/login
      register: conjur_login_response
      no_log: true

    - name: Authenticate to Conjur (get session token)
      shell: >
        curl -s -k --data '{{ conjur_login_response.stdout }}'
        https://{{ conjur_host }}/authn/{{ conjur_account }}/{{ conjur_username }}/authenticate
      register: conjur_session_token_raw
      no_log: true

    - name: Base64 encode session token using Ansible filter
      set_fact:
        conjur_session_token_b64: "{{ conjur_session_token_raw.stdout | b64encode }}"

    # Step 6: List Conjur resources (verify token)
    - name: Call Conjur API to list all resources (conjur list)
      shell: >
        curl -s -k -H "Authorization: Token token=\"{{ conjur_session_token_b64 }}\""
        https://{{ conjur_host }}/resources/{{ conjur_account }}
      register: conjur_list_output

    - name: Parse and extract Conjur resource IDs
      set_fact:
        conjur_resources: "{{ conjur_list_output.stdout | from_json | map(attribute='id') | list }}"

    - name: Show full Conjur resource list (formatted line by line)
      debug:
        var: conjur_resources

    # Step 7: Render and load host identity policy
    - name: Render host policy from Jinja template
      template:
        src: "{{ policy_template_file }}"
        dest: "{{ policy_rendered_file }}"

    - name: Read rendered host policy file
      slurp:
        src: "{{ policy_rendered_file }}"
      register: policy_file_content

    - name: Load host policy into Conjur (POST)
      uri:
        url: "https://{{ conjur_host }}/policies/{{ conjur_account }}/policy/root"
        method: POST
        headers:
          Authorization: "Token token=\"{{ conjur_session_token_b64 }}\""
          Content-Type: "application/x-yaml"
        body: "{{ policy_file_content['content'] | b64decode }}"
        body_format: raw
        validate_certs: false
      register: policy_load_response
      failed_when: policy_load_response.status not in [200, 201]
      ignore_errors: true

    # Step 8: Confirm host identity created
    - name: Check if new host was added
      set_fact:
        new_host_id: "app-path/system:serviceaccount:{{ k8s_namespace }}:{{ service_account }}"

    - name: Validate host creation
      debug:
        msg: "✅ K8s2Conjur ✅ Host '{{ new_host_id }}' was successfully registered in Conjur. ✅"
      when: new_host_id in conjur_resources | join(',')

    # Step 9: Generate variable & grant policy
    - name: Render Conjur variable+grant policy from template
      template:
        src: "{{ variable_policy_template_file }}"
        dest: "{{ variable_policy_rendered_file }}"

    - name: Read rendered variable policy file
      slurp:
        src: "{{ variable_policy_rendered_file }}"
      register: variable_policy_file_content

    - name: Load secrets variable & access policy
      uri:
        url: "https://{{ conjur_host }}/policies/{{ conjur_account }}/policy/root"
        method: POST
        headers:
          Authorization: "Token token=\"{{ conjur_session_token_b64 }}\""
          Content-Type: "application/x-yaml"
        body: "{{ variable_policy_file_content['content'] | b64decode }}"
        body_format: raw
        validate_certs: false
      register: secret_policy_result
      failed_when: secret_policy_result.status not in [200, 201]

    # NOU:Step 9.1 (AFTER variable creation): Upload secret values to Conjur
    - name: Upload secret values to Conjur
      uri:
        url: "https://{{ conjur_host }}/secrets/{{ conjur_account }}/variable/secrets/{{ item.key }}"
        method: POST
        headers:
          Authorization: "Token token=\"{{ conjur_session_token_b64 }}\""
          Content-Type: "text/plain"
        body: "{{ item.value | b64decode }}"
        body_format: raw
        validate_certs: false
      loop: >-
        {{ conjur_prefixed_secret_map | dict2items
                                     | rejectattr('value', 'equalto', '')
                                     | rejectattr('key', 'search', 'conjur-map$')
                                     | list }}
      loop_control:
        label: "{{ item.key }}"
      register: secret_upload_result
      failed_when: secret_upload_result.status not in [200, 201]
      
    - name: Show uploaded secrets result
      debug:
        msg: |
          ✅ K8s2Conjur ✅ Secret uploaded: {{ item.item.key }} → Status: {{ item.status }}
      loop: "{{ secret_upload_result.results }}"
      when: item.status in [200, 201]

    # Step 10: Patch deployment to inject Secrets Provider

    - name: Render Secrets Provider push-to-file patch
      template:
        src: "{{ secrets_provider_patch_template_file }}"
        dest: "{{ secrets_provider_config_file }}"

    - name: Patch application with Secrets Provider
      kubernetes.core.k8s:
        state: present
        host: "https://{{ ocp_api_host }}"
        api_key: "{{ ocp_token }}"
        verify_ssl: false
        src: "{{ secrets_provider_config_file }}"

    - name: Upload secret values to Conjur
      uri:
        url: "https://{{ conjur_host }}/secrets/{{ conjur_account }}/variable/secrets/{{ item.key }}"
        method: POST
        headers:
          Authorization: "Token token=\"{{ conjur_session_token_b64 }}\""
          Content-Type: "text/plain"
        body: "{{ item.value | b64decode }}"
        body_format: raw
        validate_certs: false
      loop: >-
        {{ conjur_prefixed_secret_map | dict2items
                                     | rejectattr('value', 'equalto', '')
                                     | rejectattr('key', 'search', 'conjur-map$')
                                     | list }}
      loop_control:
        label: "{{ item.key }}"
      register: secret_upload_result
      failed_when: secret_upload_result.status not in [200, 201]


    
    # Step 10.3: Deploy Reloader using Helm CLI (shell fallback)

    - name: Install Helm CLI temporarily inside EE (if not present)
      shell: |
        curl -sSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
      args:
        executable: /bin/bash

    - name: Add Stakater Helm repo and update
      shell: |
        helm repo add stakater https://stakater.github.io/stakater-charts
        helm repo update
      args:
        executable: /bin/bash

    - name: Detect if running on OpenShift using Kubernetes API
      kubernetes.core.k8s_info:
        kind: ClusterVersion
        api_version: config.openshift.io/v1
        name: version
        host: "https://{{ ocp_api_host }}"
        api_key: "{{ ocp_token }}"
        verify_ssl: false
      register: cluster_version_result
      failed_when: false
      changed_when: false

    - name: Set cluster type fact
      set_fact:
        is_openshift: "{{ cluster_version_result.resources is defined and cluster_version_result.resources | length > 0 }}"

    - name: Get namespace metadata (OpenShift only)
      when: is_openshift
      kubernetes.core.k8s_info:
        kind: Namespace
        name: "{{ k8s_namespace }}"
        host: "https://{{ ocp_api_host }}"
        api_key: "{{ ocp_token }}"
        verify_ssl: false
      register: namespace_info

    - name: Extract UID range and compute runAsUser
      when: >
        is_openshift and
        (namespace_info.resources[0].metadata.annotations is defined and
         'openshift.io/sa.scc.uid-range' in namespace_info.resources[0].metadata.annotations)
      block:
        - name: Extract UID range safely
          set_fact:
            uid_range: "{{ namespace_info.resources[0].metadata.annotations['openshift.io/sa.scc.uid-range'] }}"

        - name: Compute runAsUser from UID range
          set_fact:
            run_as_uid: "{{ (uid_range.split('/')[0] | int) + 1 }}"


    - name: Set default runAsUser if UID range not found
      set_fact:
        run_as_uid: 1000
      when: run_as_uid is not defined

    - name: Set Helm flags for Reloader based on cluster type
      set_fact:
        helm_flags: >-
          {% if is_openshift %}
          --set reloader.isOpenshift=true
          --set reloader.deployment.securityContext.runAsUser={{ run_as_uid }}
          --set reloader.deployment.securityContext.runAsNonRoot=true
          --set reloader.deployment.securityContext.seccompProfile.type=RuntimeDefault
          {% else %}
          --set reloader.deployment.securityContext.runAsUser=1000
          --set reloader.deployment.securityContext.runAsNonRoot=true
          --set reloader.deployment.securityContext.seccompProfile.type=RuntimeDefault
          {% endif %}

    - name: Install Reloader via Helm
      shell: |
        helm upgrade --install reloader stakater/reloader \
          --namespace "{{ k8s_namespace }}" \
          --create-namespace \
          --kube-insecure-skip-tls-verify \
          --set reloader.watchGlobally=false \
          --set reloader.namespaces={{ k8s_namespace }} \
          {{ helm_flags }} \
          --kube-token="{{ ocp_token }}" \
          --kube-apiserver="https://{{ ocp_api_host }}"
      args:
        executable: /bin/bash

    #- name: Install Reloader via Helm with SCC-compliant UID
      #shell: |
        #helm upgrade --install reloader stakater/reloader \
          #--kube-insecure-skip-tls-verify \
          #--namespace reloader \
          #--create-namespace \
          #--set reloader.watchGlobally=false \
          #--set reloader.namespaces={{ k8s_namespace }} \
          #--set securityContext.runAsUser={{ run_as_uid }} \
          #--set securityContext.runAsNonRoot=true \
          #--set securityContext.seccompProfile.type="RuntimeDefault" \
          #--kube-token="{{ ocp_token }}" \
          #--kube-apiserver="https://{{ ocp_api_host }}"
      #args:
        #executable: /bin/bash


    # Step 10.4: Patch deployment with Reloader annotations
    - name: Patch deployment with Reloader annotation to auto-reload on secret change @etudurd
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: "{{ deployment_name }}"
            namespace: "{{ k8s_namespace }}"
            annotations:
              reloader.stakater.com/auto: "true"
        host: "https://{{ ocp_api_host }}"
        api_key: "{{ ocp_token }}"
        verify_ssl: false
    
    # Step 11: Show policy and patch results
    - name: Show policy load result (final status)
      debug:
        msg: >
          📜 K8s2Conjur 📜 Final policy load status:
          - Host policy POST: {{ policy_load_response.status | default('N/A') }}
          - Variable policy POST: {{ secret_policy_result.status | default('N/A') }}

    - name: CLI equivalent commands
      debug:
        msg: |
          💡 Equivalent CLI:
          conjur policy load -f {{ policy_rendered_file }} -b root
          conjur policy load -f {{ variable_policy_rendered_file }} -b root
    
    #am adaugat un rezumat cu emoji sa arate mai bine din punct de vedere al utilizatorului @etudurd
    - name: ✅ K8s2Conjur Rezumat 
      debug:
        msg: "{{ summary_message.split('\n') }}"
      vars:
         summary_message: |
           - "PUSH TO FILE LOADED BETA" 
           - "C 🔍 Application '{{ deployment_name }}' scanned for Kubernetes Secrets"
           - "Y 🛡️ Host identity '{{ new_host_id }}' onboarded to Conjur"
           - "B 🔐 {{ secret_kv_pairs | length }} secret(s) extracted and onboarded to Conjur"
           - "E 📜 Host & variable policies created and loaded (HTTP {{ policy_load_response.status }} / {{ secret_policy_result.status }})"
           - "R 🚀 Secrets Provider sidecar injected into the Kubernetes Deployment"
           - "A 🔁 Role and RoleBinding for secret access successfully created in namespace '{{ k8s_namespace }}'"
           - "  🌀 Reloader Controller deployed 1.1"
           - "R == JWT Authentication enabled =="
           - "K ✅ K8s2Conjur Automation Completed Successfully ✅"
           
